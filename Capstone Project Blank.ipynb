{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Accuracy and Precision of Sentiment Analysis Over Ensemble Supervised Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIM\n",
    "Sentiment analysis is a machine learning-based technique used to analyze subjective reviews in a computer-understandable language. Our project aims to compare various ML models for performing sentiment analysis. We used an umbrella dataset that combined three popular datasets to increase the diversity of training and testing data. In this project notebook, we have designed and tested multiple supervised machine learning algorithms such as - Random Forest, Extra Tree Classifier, Decision Tree, Logistic Regression, and XG Boosting. Through our experiments, we were able to showcase how other algorithms report a better performance for the use cases we have presented. We also designed a Voting Classifier that uses a majority voting method to aggregate the results obtained by all the other models and combine them to report its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO USE THIS PROJECT\n",
    "The notebook has been divided into 5 main catogories - Loading Datasets and Libraries, Dataset Processing, Visualization, Algorithms, and Results and Discussion. The client using the notebook to perform sentiment analysis is required to follow the steps in order to ensure proper functioning. The detailed information regarding each step is given as the notebook reads. The client is open to jump to any step for understanding purposes only. We highly recommend to run the cells in order for smooth functioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 - Loading Dataset and Libraries\n",
    "The program requires use of multiple libraries which need to be imported into your system for use. Therefore, client needs to open a terminal window and install the libraries not present in the system using. You can refer to the following packaging guide to understand the process of installing libraries - https://packaging.python.org/en/latest/tutorials/installing-packages/\n",
    "\n",
    "To check the current list of libraries and their versions, client can use the following guide - https://www.activestate.com/resources/quick-reads/how-to-list-installed-python-packages/\n",
    "\n",
    "Once all the package requirements are fullfiled, client can run the first set of code cells under Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Necessary Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import os\n",
    "\n",
    "import spacy \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import folium\n",
    "from itertools import cycle, islice\n",
    "from pandas import options\n",
    "import warnings\n",
    "import pickle\n",
    "import nltk\n",
    "from matplotlib.pyplot import figure\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "\n",
    "# Downloading Special Packages Under Libraries\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "\n",
    "from  wordcloud import WordCloud\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 - Dataset Processing\n",
    "Our test dataset is currently loaded in the Jupyter Notebook. However, the process to replace it with your own dataset is very simple, the client needs to follow a few simple steps.\n",
    "\n",
    "First - Prepare the dataset. Make sure that client dataset has only two columns. Column 1 contaians the text reviews and the column heading has to be \"review\". Column 2 contains the sentiment value denaoted as 0 for negative and 1 for positive only. Remove all other columns form the dataset and store as CSV file.\n",
    "\n",
    "If client sentiment value is not in the given format, then client will need to apply simple python commands to make the necessary changes. Use the following guide for help - https://www.educative.io/blog/pandas-cheat-sheet\n",
    "\n",
    "Second - Make sure client CSV dataset file is saved in the same folder as this notebook, otherwise client will need to change the command for loading dataset.\n",
    "\n",
    "Third - Now client can copy and paste the name of file including the extension in the first cell under this step as mentioned.\n",
    "\n",
    "Fourth - No more changes required, just run all the following cells and wait for completion. If any errors/problems are raised, use google for simple troubleshooting and double ckeck if all previous steps were followed properly. Client can also contact us via email - guptaabhinav2205@gmail.com or somilkuchhal.kuchhal@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the name of your final dataset as: './datasets/aclimdb.csv'\n",
    "\n",
    "DATA_SET = './FinalDataset.csv'\n",
    "df =pd.read_csv(DATA_SET, index_col = 0)\n",
    "df.review = df.review.astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all text review to lower case\n",
    "\n",
    "df['review']=df['review'].apply(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the cont_to_exp() and a dictionary:{key: contractions,value:expansion}\n",
    "# Converting contractions used in text reviews to full forms\n",
    "# For eg. he'll --> he will\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how does\",\"i'd\": \"i would\",\"i'd've\": \"i would have\",\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\"she's\": \"she is\", \"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so is\", \"that'd\": \"that would\",\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\", \"there'd\": \"there would\",\"there'd've\": \"there would have\",\"there's\": \"there is\", \"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\n",
    "\"they've\": \"they have\", \"to've\": \"to have\",\"wasn't\": \"was not\",\" u \": \" you \",\" ur \": \" your \",\" n \": \" and \",\n",
    "\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "def cont_to_exp(x):\n",
    "    if type(x) is str:\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key,value)\n",
    "        return x\n",
    "    else : \n",
    "        return x\n",
    "df['review'] = df['review'].apply(lambda x:cont_to_exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the urls from the reviews\n",
    "\n",
    "df['review']=df['review'].apply(lambda x: re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?','',x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword removal : Stopwords are the words that appear quite frequently in a sentence and do not have a significant \n",
    "# contribution to the meaning of the sentence. Therefore they can be removed. \n",
    "\n",
    "df['review'] = df['review'].apply(lambda x:\" \".join([t for t in x.split() if t not in STOP_WORDS ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of special characters from the reviews\n",
    "\n",
    "df['review']=df['review'].apply(lambda x:re.sub(r'[^0-9a-zA-Z *]','',x))\n",
    "df['review']=df['review'].apply(lambda x:re.sub(r'[^a-zA-z0-9\\s]','',x))\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of mulitple spaces between the words in the review\n",
    "\n",
    "df[\"review\"]=df[\"review\"].apply(lambda x: \" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of HTML Tags: from the reviews\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x:BeautifulSoup(x,'lxml').get_text())\n",
    "\n",
    "# Removal of tags and links \n",
    "\n",
    "tag = re.compile(r'<[^>]+>')\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: tag.sub('', x)) #removing html labels\n",
    "\n",
    "df['review'] = df['review'].replace(r'http\\S+', '', regex=True).replace(r'www.\\S+', '', regex=True).replace(r'http\\S+', '', regex=True).replace(r'\"', '', regex=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Numbers\n",
    "\n",
    "df['review']=df['review'].apply(lambda x:re.sub(r'[0-9]+','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of usernames from the reviews\n",
    "\n",
    "df['review']=df['review'].apply(lambda x:re.sub(r'@[A-Za-z0â€“9]+','',x))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tokenization and Lemmanization\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(w, pos=\"v\") for w in w_tokenizer.tokenize(text)])\n",
    "\n",
    "df['review'] = df.review.apply(lemmatize_text).copy()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Spliting Libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "x = pd.DataFrame(df, columns = ['review']) \n",
    "y = pd.DataFrame(df, columns = ['sentiment']) \n",
    "\n",
    "# Split dataset into Training and Testing Data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Shape of x_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of x_test:  \", X_test.shape)\n",
    "print(\"Shape of y_test:  \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization with TF-IDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train = X_train['review'].tolist()\n",
    "test = X_test['review'].tolist()\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, dtype=np.float32)\n",
    "\n",
    "tfidfX_train = tfidf_vectorizer.fit_transform(train)\n",
    "tfidfX_train = tfidfX_train.toarray()\n",
    "\n",
    "tfidfX_test = tfidf_vectorizer.transform(test)\n",
    "tfidfX_test = tfidfX_test.toarray()\n",
    "\n",
    "print(\"TF-IDF train shape:\", tfidfX_train.shape)\n",
    "print(\"TF-IDF test shape:\", tfidfX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 - Visualization\n",
    "In this project, we have also focused on understanding our dataset. The word cloud visualization library of python helps us in doing so by creating pictorial representation of most used words and phrases. \n",
    "\n",
    "All the code cells have been designed to work seemlessly, the client all needs to run the cells wihtout making any edits. However, kindly note that this library requires a little extra time to process the data based on client system capabilities.\n",
    "\n",
    "Once the cells have run, the client should be able to see 2 separate word clouds representing the positive and negative most used words based on the dataset and which rows have been marked +ve and -ve sentiments.\n",
    "\n",
    "Note: This step does not affect the results of sentiment analysis. It was added as an extra educational step to help gain a better understanding of the data being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud Visualization \n",
    "\n",
    "df_positive = df[df['sentiment'] == 1]\n",
    "bag_of_words_positive =' '.join(df_positive['review'])\n",
    "bag_of_words_positive = bag_of_words_positive.split()\n",
    "\n",
    "df_negative = df[df['sentiment'] == 0]\n",
    "bag_of_words_negative =' '.join(df_negative['review'])\n",
    "bag_of_words_negative = bag_of_words_negative.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = ' '.join(bag_of_words_positive[:20000])\n",
    "len(bag_of_words_positive)\n",
    "print(x)\n",
    "\n",
    "y = ' '.join(bag_of_words_negative[:20000])\n",
    "len(bag_of_words_negative)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive Word Cloud \n",
    "\n",
    "from PIL import Image\n",
    "plt.figure(figsize=(12,8))\n",
    "wc = WordCloud(background_color=\"white\",width=1800,height=1400).generate(x)\n",
    "plt.imshow(wc.recolor(colormap='Dark2', random_state=17), alpha=0.98)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Word Cloud \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "wc = WordCloud(background_color=\"white\",width=1800,height=1400).generate(y)\n",
    "plt.imshow(wc.recolor(colormap='Dark2', random_state=17), alpha=0.98)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 - Algorithms\n",
    "Now we have the most interesting and important step of the process, the actual algorithms that will perform sentiment analysis on the data. In this section, the algorithms have been designed in a way to run without interference. The client has to run the cells wait for results to be generated based on their data.\n",
    "\n",
    "If any errors/problems are raised, use google for simple troubleshooting and double ckeck if all previous steps were followed properly. However, a case as such will rarely occur. Client can also contact us via email - guptaabhinav2205@gmail.com or somilkuchhal.kuchhal@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dct = DecisionTreeClassifier(criterion='entropy', random_state=1)\n",
    "dct.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "\n",
    "y_pred_dct = dct.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "dct_accuracy = accuracy_score(y_test,y_pred_dct)*100\n",
    "dct_matrix = confusion_matrix(y_test,y_pred_dct)\n",
    "dct_precision = dct_matrix[0][0]*100/(dct_matrix[0][0]+dct_matrix[1][0])\n",
    "dct_recall = dct_matrix[0][0]*100/(dct_matrix[0][0]+dct_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",dct_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",dct_matrix)\n",
    "print(\"precision:\",dct_precision)\n",
    "print(\"recall:\",dct_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xg = XGBClassifier(random_state=22,learning_rate=0.9)\n",
    "xg.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "y_pred_xg = xg.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "xg_accuracy = accuracy_score(y_test,y_pred_xg)*100\n",
    "xg_matrix = confusion_matrix(y_test,y_pred_xg)\n",
    "xg_precision = xg_matrix[0][0]*100/(xg_matrix[0][0]+xg_matrix[1][0])\n",
    "xg_recall = xg_matrix[0][0]*100/(xg_matrix[0][0]+xg_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",xg_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",xg_matrix)\n",
    "print(\"precision:\",xg_precision)\n",
    "print(\"recall:\",xg_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=150,max_depth=None)\n",
    "rf1.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "\n",
    "y_pred_rf1 = rf1.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "rf1_accuracy = accuracy_score(y_test,y_pred_rf1)*100\n",
    "rf1_matrix = confusion_matrix(y_test,y_pred_rf1)\n",
    "rf1_precision = rf1_matrix[0][0]*100/(rf1_matrix[0][0]+rf1_matrix[1][0])\n",
    "rf1_recall = rf1_matrix[0][0]*100/(rf1_matrix[0][0]+rf1_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",rf1_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",rf1_matrix)\n",
    "print(\"precision:\",rf1_precision)\n",
    "print(\"recall:\",rf1_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0,solver='lbfgs')\n",
    "\n",
    "lr.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "\n",
    "y_pred_lr = lr.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test,y_pred_lr)*100\n",
    "lr_matrix = confusion_matrix(y_test,y_pred_lr)\n",
    "lr_precision = lr_matrix[0][0]*100/(lr_matrix[0][0]+lr_matrix[1][0])\n",
    "lr_recall = lr_matrix[0][0]*100/(lr_matrix[0][0]+lr_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",lr_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",lr_matrix)\n",
    "print(\"precision:\",lr_precision)\n",
    "print(\"recall:\",lr_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier(random_state=123)\n",
    "etc.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "\n",
    "y_pred_etc = etc.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "etc_accuracy = accuracy_score(y_test,y_pred_etc)*100\n",
    "etc_matrix = confusion_matrix(y_test,y_pred_etc)\n",
    "etc_precision = etc_matrix[0][0]*100/(etc_matrix[0][0]+etc_matrix[1][0])\n",
    "etc_recall = etc_matrix[0][0]*100/(etc_matrix[0][0]+etc_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",etc_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",etc_matrix)\n",
    "print(\"precision:\",etc_precision)\n",
    "print(\"recall:\",etc_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Estimators\n",
    "\n",
    "estimators = [('dct',dct),('xg',xg),('rf1',rf1),('lr',lr),('etc',etc)]\n",
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Estimator\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vc = VotingClassifier(estimators, weights=[0.5,1,2.5,1,2.5])\n",
    "vc.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "\n",
    "y_pred_vc = vc.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "vc_accuracy = accuracy_score(y_test,y_pred_vc)*100\n",
    "vc_matrix = confusion_matrix(y_test,y_pred_vc)\n",
    "vc_precision = vc_matrix[0][0]*100/(vc_matrix[0][0]+vc_matrix[1][0])\n",
    "vc_recall = vc_matrix[0][0]*100/(vc_matrix[0][0]+vc_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",vc_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",vc_matrix)\n",
    "print(\"precision:\",vc_precision)\n",
    "print(\"recall:\",vc_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 - Results and Discussion\n",
    "Even though all the algorithms report their individual results, it is very difficult to compare and contrast in given format. Therrefore, we have tabulated the results obtained from all the other algorithms in a single code for easier working. The table reports the accuracy, precision and recall of all algorithms.\n",
    "\n",
    "The client can compare and contrast the performance of all the different algorithms based on their dataset and choose the right model ahead. We have also created a few graphs that will be generated automatically upon running the code. The graphs also draw attention towards the variation in accuracy, precision and recall values.\n",
    "\n",
    "\n",
    "Kindly note that this project works as a baseline model to help understand the working of different algorithms on any dataset. There is always scope for additional improvement and enhancement of any model. The client can use this project to select one model out of many to work upon with further scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Results in Tabular Format for Easy Understanding\n",
    "\n",
    "print(\"Testing Accuracies\")\n",
    "acc_list = {\n",
    "    'Decision Tree':dct_accuracy,\n",
    "    'XG':xg_accuracy,\n",
    "    'Random Forest':rf1_accuracy,\n",
    "    'Logistic Regression':lr_accuracy,\n",
    "    'Extra Tree Classifier':etc_accuracy,\n",
    "    'Voting Classifier':vc_accuracy\n",
    "}\n",
    "\n",
    "precision_list = {\n",
    "    'Decision Tree':dct_precision,\n",
    "    'XG':xg_precision,\n",
    "    'Random Forest':rf1_precision,\n",
    "    'Logistic Regression':lr_precision,\n",
    "    'Extra Tree Classifier':etc_precision,\n",
    "    'Voting Classifier':vc_precision\n",
    "}\n",
    "\n",
    "recall_list={\n",
    "    'Decision Tree':dct_recall,\n",
    "    'XG':xg_recall,\n",
    "    'Random Forest':rf1_recall,\n",
    "    'Logistic Regression':lr_recall,\n",
    "    'Extra Tree Classifier':etc_recall,\n",
    "    'Voting Classifier':vc_recall\n",
    "    \n",
    "}\n",
    "\n",
    "acc_df_test = pd.DataFrame.from_dict(acc_list,orient=\"index\",columns=['Accuracy'])\n",
    "prec_df_test = pd.DataFrame.from_dict(precision_list,orient=\"index\",columns=['Precision'])\n",
    "f1_df_test = pd.DataFrame.from_dict(recall_list,orient=\"index\",columns=['Recall'])\n",
    "df_test = pd.concat([acc_df_test,prec_df_test,f1_df_test],axis = 1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generating Graph of Accuracy vs Models\n",
    "\n",
    "models = list(acc_list.keys())\n",
    "acc = list(acc_list.values())\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 5), sharey=True)\n",
    "plt.ylabel(\"Accuracy\", labelpad=15, fontdict={'size':14})\n",
    "plt.xlabel(\"Models\", labelpad=15, fontdict={'size':14}, rotation=1)\n",
    "\n",
    "axs.tick_params(axis='x', rotation=10, labelsize=12)\n",
    "axs.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "axs.plot(models, acc)\n",
    "fig.suptitle('Compairing Accuracies', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Graph of Precision vs Models\n",
    "\n",
    "models = list(precision_list.keys())\n",
    "prec = list(precision_list.values())\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 5), sharey=True)\n",
    "plt.ylabel(\"Precision\", labelpad=15, fontdict={'size':14})\n",
    "plt.xlabel(\"Models\", labelpad=15, fontdict={'size':14}, rotation=1.5)\n",
    "\n",
    "axs.tick_params(axis='x', rotation=10, labelsize=12)\n",
    "axs.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "axs.plot(models, prec)\n",
    "fig.suptitle('Compairing Precision', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Graph of Recall vs Models\n",
    "\n",
    "models = list(recall_list.keys())\n",
    "recall = list(recall_list.values())\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 5), sharey=True)\n",
    "plt.ylabel(\"Recall\", labelpad=15, fontdict={'size':14})\n",
    "plt.xlabel(\"Models\", labelpad=15, fontdict={'size':14}, rotation=1.5)\n",
    "\n",
    "axs.tick_params(axis='x', rotation=10, labelsize=12)\n",
    "axs.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "axs.plot(models, recall)\n",
    "fig.suptitle('Compairing Recall', fontsize=18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
