{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Literature Survey 1 - Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we run our project using a dataset of Youtube Comments acquired through Kaggle. We found this dataset to be very similar to the one used in our literature survey. By testing our models, we can draw a comparison between our performance and that of Naive Bayes reported in the research papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset and Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Abhinav\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhinav\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import os\n",
    "\n",
    "import spacy \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import folium\n",
    "from itertools import cycle, islice\n",
    "from pandas import options\n",
    "import warnings\n",
    "import pickle\n",
    "import nltk\n",
    "from matplotlib.pyplot import figure\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "\n",
    "from  wordcloud import WordCloud\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_SET = './datasets/aclimdb.csv'\n",
    "DATA_SET = './YoutubeComments.csv'\n",
    "df =pd.read_csv(DATA_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Let's not forget that Apple Pay in 2014 requir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here in NZ 50% of retailers don’t even have co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I will forever acknowledge this channel with t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whenever I go to a place that doesn’t take App...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple Pay is so convenient, secure, and easy t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18404</th>\n",
       "      <td>I really like the point about engineering tool...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18405</th>\n",
       "      <td>I’ve just started exploring this field. And th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18406</th>\n",
       "      <td>Excelente video con una pregunta filosófica pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18407</th>\n",
       "      <td>Hey Daniel, just discovered your channel a cou...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18408</th>\n",
       "      <td>This is great. Focus is key. A playful approac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      Let's not forget that Apple Pay in 2014 requir...          1\n",
       "1      Here in NZ 50% of retailers don’t even have co...          0\n",
       "2      I will forever acknowledge this channel with t...          2\n",
       "3      Whenever I go to a place that doesn’t take App...          0\n",
       "4      Apple Pay is so convenient, secure, and easy t...          2\n",
       "...                                                  ...        ...\n",
       "18404  I really like the point about engineering tool...          2\n",
       "18405  I’ve just started exploring this field. And th...          2\n",
       "18406  Excelente video con una pregunta filosófica pr...          1\n",
       "18407  Hey Daniel, just discovered your channel a cou...          2\n",
       "18408  This is great. Focus is key. A playful approac...          2\n",
       "\n",
       "[18409 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review = df.review.astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.replace(2, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>let's not forget that apple pay in 2014 requir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50% of retailers don’t even have co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient, secure, and easy t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  let's not forget that apple pay in 2014 requir...          1\n",
       "1  here in nz 50% of retailers don’t even have co...          0\n",
       "2  i will forever acknowledge this channel with t...          1\n",
       "3  whenever i go to a place that doesn’t take app...          0\n",
       "4  apple pay is so convenient, secure, and easy t...          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lowercasing all the words in the review\n",
    "df['review']=df['review'].apply(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contraction to expansion : \n",
    "#converting the words in their contracted form to their extracted form eg. he'll to he will\n",
    "#using the cont_to_exp() and a dictionary:{key: contractions,value:expansion}\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\"wasn't\": \"was not\",\n",
    "\" u \": \" you \",\" ur \": \" your \",\" n \": \" and \",\n",
    "\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \n",
    "\"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n",
    "\"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "def cont_to_exp(x):\n",
    "    if type(x) is str:\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key,value)\n",
    "        return x\n",
    "    else : \n",
    "        return x\n",
    "df['review'] = df['review'].apply(lambda x:cont_to_exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the urls from the reviews\n",
    "df['review']=df['review'].apply(lambda x: re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?','',x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword removal : Stopwords are the words that appear quite frequently in a sentence and do not have a significant contribution to the meaning of the sentence. Therefore they can be removed.\n",
    "df['review'] = df['review'].apply(lambda x:\" \".join([t for t in x.split() if t not in STOP_WORDS ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of special characters from the reviews\n",
    "df['review']=df['review'].apply(lambda x:re.sub(r'[^0-9a-zA-Z *]','',x))\n",
    "df['review']=df['review'].apply(lambda x:re.sub(r'[^a-zA-z0-9\\s]','',x))\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of mulitple spaces between the words in the review\n",
    "df[\"review\"]=df[\"review\"].apply(lambda x: \" \".join(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>let forget apple pay 2014 required brand new i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nz 50 retailers dont contactless credit card m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forever acknowledge channel help lessons ideas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place doesnt apple pay doesnt happen often its...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay convenient secure easy use korean ja...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18404</th>\n",
       "      <td>like point engineering toolboxes think thats l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18405</th>\n",
       "      <td>ive started exploring field good reminder earl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18406</th>\n",
       "      <td>excelente video con una pregunta filosfica pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18407</th>\n",
       "      <td>hey daniel discovered channel couple days ago ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18408</th>\n",
       "      <td>great focus key playful approach speed things ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      let forget apple pay 2014 required brand new i...          1\n",
       "1      nz 50 retailers dont contactless credit card m...          0\n",
       "2      forever acknowledge channel help lessons ideas...          1\n",
       "3      place doesnt apple pay doesnt happen often its...          0\n",
       "4      apple pay convenient secure easy use korean ja...          1\n",
       "...                                                  ...        ...\n",
       "18404  like point engineering toolboxes think thats l...          1\n",
       "18405  ive started exploring field good reminder earl...          1\n",
       "18406  excelente video con una pregunta filosfica pro...          1\n",
       "18407  hey daniel discovered channel couple days ago ...          1\n",
       "18408  great focus key playful approach speed things ...          1\n",
       "\n",
       "[18409 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removal of HTML Tags: from the reviews\n",
    "df['review'] = df['review'].apply(lambda x:BeautifulSoup(x,'lxml').get_text())\n",
    "\n",
    "#Remove tags and links \n",
    "tag = re.compile(r'<[^>]+>')\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: tag.sub('', x)) #removing html labels\n",
    "\n",
    "df['review'] = df['review'].replace(r'http\\S+', '', regex=True).replace(r'www.\\S+', '', regex=True).replace(r'http\\S+', '', regex=True).replace(r'\"', '', regex=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Numbers\n",
    "df['review']=df['review'].apply(lambda x:re.sub(r'[0-9]+','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9593</th>\n",
       "      <td>mee goreng vibe definitely childhood faves awe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13362</th>\n",
       "      <td>hybrid warfare is terrorism simple plain cowar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15610</th>\n",
       "      <td>st console original ps loved stayed upgraded p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>spot maintenance number  owned trucking compan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13228</th>\n",
       "      <td>wish guests answer questionsand points</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4945</th>\n",
       "      <td>excited believe focused iphone apple watch may...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>said packaging affecting taste true think deli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12114</th>\n",
       "      <td>im worried cant sat august tt techniques please</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>a vida to bela alis o nosso planeta terra tem ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10100</th>\n",
       "      <td>numbers screen climbing gforce g stands gravi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "9593   mee goreng vibe definitely childhood faves awe...          1\n",
       "13362  hybrid warfare is terrorism simple plain cowar...          0\n",
       "15610  st console original ps loved stayed upgraded p...          1\n",
       "2319   spot maintenance number  owned trucking compan...          1\n",
       "13228             wish guests answer questionsand points          1\n",
       "4945   excited believe focused iphone apple watch may...          1\n",
       "285    said packaging affecting taste true think deli...          1\n",
       "12114    im worried cant sat august tt techniques please          0\n",
       "10998  a vida to bela alis o nosso planeta terra tem ...          1\n",
       "10100   numbers screen climbing gforce g stands gravi...          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removal of usernames from the reviews\n",
    "df['review']=df['review'].apply(lambda x:re.sub(r'@[A-Za-z0–9]+','',x))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Abhinav\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>let forget apple pay require brand new iphone ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nz retailers dont contactless credit card mach...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forever acknowledge channel help lessons ideas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place doesnt apple pay doesnt happen often its...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay convenient secure easy use korean ja...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18404</th>\n",
       "      <td>like point engineer toolboxes think thats lot ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18405</th>\n",
       "      <td>ive start explore field good reminder early jo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18406</th>\n",
       "      <td>excelente video con una pregunta filosfica pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18407</th>\n",
       "      <td>hey daniel discover channel couple days ago le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18408</th>\n",
       "      <td>great focus key playful approach speed things ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      let forget apple pay require brand new iphone ...          1\n",
       "1      nz retailers dont contactless credit card mach...          0\n",
       "2      forever acknowledge channel help lessons ideas...          1\n",
       "3      place doesnt apple pay doesnt happen often its...          0\n",
       "4      apple pay convenient secure easy use korean ja...          1\n",
       "...                                                  ...        ...\n",
       "18404  like point engineer toolboxes think thats lot ...          1\n",
       "18405  ive start explore field good reminder early jo...          1\n",
       "18406  excelente video con una pregunta filosfica pro...          1\n",
       "18407  hey daniel discover channel couple days ago le...          1\n",
       "18408  great focus key playful approach speed things ...          1\n",
       "\n",
       "[18409 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization and Lemmanization\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(w, pos=\"v\") for w in w_tokenizer.tokenize(text)])\n",
    "\n",
    "df['review'] = df.review.apply(lemmatize_text).copy()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train:  (13806, 1)\n",
      "Shape of y_train:  (13806, 1)\n",
      "Shape of x_test:   (4603, 1)\n",
      "Shape of y_test:   (4603, 1)\n"
     ]
    }
   ],
   "source": [
    "# Splitting Dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "x = pd.DataFrame(df, columns = ['review']) \n",
    "y = pd.DataFrame(df, columns = ['sentiment']) \n",
    "\n",
    "# Split dataset to train and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Shape of x_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of x_test:  \", X_test.shape)\n",
    "print(\"Shape of y_test:  \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization with TF-IDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF train shape: (13806, 1000)\n",
      "TF-IDF test shape: (4603, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train = X_train['review'].tolist()\n",
    "test = X_test['review'].tolist()\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, dtype=np.float32)\n",
    "\n",
    "tfidfX_train = tfidf_vectorizer.fit_transform(train)\n",
    "tfidfX_train = tfidfX_train.toarray()\n",
    "\n",
    "tfidfX_test = tfidf_vectorizer.transform(test)\n",
    "tfidfX_test = tfidfX_test.toarray()\n",
    "\n",
    "print(\"TF-IDF train shape:\", tfidfX_train.shape)\n",
    "print(\"TF-IDF test shape:\", tfidfX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dct = DecisionTreeClassifier(criterion='entropy', random_state=1)\n",
    "dct.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  83.38040408429285\n",
      "Confusion_matrix:\n",
      " [[ 179  417]\n",
      " [ 348 3659]]\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions\n",
    "y_pred_dct = dct.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "dct_accuracy = accuracy_score(y_test,y_pred_dct)*100\n",
    "dct_matrix = confusion_matrix(y_test,y_pred_dct)\n",
    "dct_precision = dct_matrix[0][0]*100/(dct_matrix[0][0]+dct_matrix[1][0])\n",
    "dct_recall = dct_matrix[0][0]*100/(dct_matrix[0][0]+dct_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",dct_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",dct_matrix)\n",
    "# print(\"precision:\",dct_precision)\n",
    "# print(\"recall:\",dct_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boosting ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.9, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=22, ...)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xg = XGBClassifier(random_state=22,learning_rate=0.9)\n",
    "xg.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  86.8563980013035\n",
      "Confusion_matrix:\n",
      " [[ 161  435]\n",
      " [ 170 3837]]\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions\n",
    "y_pred_xg = xg.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "xg_accuracy = accuracy_score(y_test,y_pred_xg)*100\n",
    "xg_matrix = confusion_matrix(y_test,y_pred_xg)\n",
    "xg_precision = xg_matrix[0][0]*100/(xg_matrix[0][0]+xg_matrix[1][0])\n",
    "xg_recall = xg_matrix[0][0]*100/(xg_matrix[0][0]+xg_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",xg_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",xg_matrix)\n",
    "# print(\"precision:\",xg_precision)\n",
    "# print(\"recall:\",xg_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABHINA~1\\AppData\\Local\\Temp/ipykernel_12116/2677021949.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf1.fit(tfidfX_train,y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=150)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=150,max_depth=None)\n",
    "rf1.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  87.46469693678036\n",
      "Confusion_matrix:\n",
      " [[  73  523]\n",
      " [  54 3953]]\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions\n",
    "y_pred_rf1 = rf1.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "rf1_accuracy = accuracy_score(y_test,y_pred_rf1)*100\n",
    "rf1_matrix = confusion_matrix(y_test,y_pred_rf1)\n",
    "rf1_precision = rf1_matrix[0][0]*100/(rf1_matrix[0][0]+rf1_matrix[1][0])\n",
    "rf1_recall = rf1_matrix[0][0]*100/(rf1_matrix[0][0]+rf1_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",rf1_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",rf1_matrix)\n",
    "# print(\"precision:\",rf1_precision)\n",
    "# print(\"recall:\",rf1_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Softwares\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0,solver='lbfgs')\n",
    "\n",
    "lr.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  87.94264610036933\n",
      "Confusion_matrix:\n",
      " [[  90  506]\n",
      " [  49 3958]]\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions\n",
    "y_pred_lr = lr.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test,y_pred_lr)*100\n",
    "lr_matrix = confusion_matrix(y_test,y_pred_lr)\n",
    "lr_precision = lr_matrix[0][0]*100/(lr_matrix[0][0]+lr_matrix[1][0])\n",
    "lr_recall = lr_matrix[0][0]*100/(lr_matrix[0][0]+lr_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",lr_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",lr_matrix)\n",
    "# print(\"precision:\",lr_precision)\n",
    "# print(\"recall:\",lr_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tree Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABHINA~1\\AppData\\Local\\Temp/ipykernel_12116/165346209.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  etc.fit(tfidfX_train,y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(random_state=123)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier(random_state=123)\n",
    "etc.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  87.26917227894852\n",
      "Confusion_matrix:\n",
      " [[  69  527]\n",
      " [  59 3948]]\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions\n",
    "y_pred_etc = etc.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "etc_accuracy = accuracy_score(y_test,y_pred_etc)*100\n",
    "etc_matrix = confusion_matrix(y_test,y_pred_etc)\n",
    "etc_precision = etc_matrix[0][0]*100/(etc_matrix[0][0]+etc_matrix[1][0])\n",
    "etc_recall = etc_matrix[0][0]*100/(etc_matrix[0][0]+etc_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",etc_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",etc_matrix)\n",
    "# print(\"precision:\",etc_precision)\n",
    "# print(\"recall:\",etc_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dct', DecisionTreeClassifier(criterion='entropy', random_state=1)),\n",
       " ('xg',\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.9, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=22, ...)),\n",
       " ('rf1', RandomForestClassifier(n_estimators=150)),\n",
       " ('lr', LogisticRegression(random_state=0)),\n",
       " ('etc', ExtraTreesClassifier(random_state=123))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining Estimators\n",
    "\n",
    "estimators = [('dct',dct),('xg',xg),('rf1',rf1),('lr',lr),('etc',etc)]\n",
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Softwares\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('dct',\n",
       "                              DecisionTreeClassifier(criterion='entropy',\n",
       "                                                     random_state=1)),\n",
       "                             ('xg',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            gpu_id=None, grow_policy=...\n",
       "                                            max_delta_step=None, max_depth=None,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            predictor=None, random_state=22, ...)),\n",
       "                             ('rf1', RandomForestClassifier(n_estimators=150)),\n",
       "                             ('lr', LogisticRegression(random_state=0)),\n",
       "                             ('etc', ExtraTreesClassifier(random_state=123))],\n",
       "                 weights=[0.5, 1, 2.5, 1, 2.5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the Estimator\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vc = VotingClassifier(estimators, weights=[0.5,1,2.5,1,2.5])\n",
    "vc.fit(tfidfX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  87.59504670866825\n",
      "Confusion_matrix:\n",
      " [[  75  521]\n",
      " [  50 3957]]\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions\n",
    "y_pred_vc = vc.predict(tfidfX_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "vc_accuracy = accuracy_score(y_test,y_pred_vc)*100\n",
    "vc_matrix = confusion_matrix(y_test,y_pred_vc)\n",
    "vc_precision = vc_matrix[0][0]*100/(vc_matrix[0][0]+vc_matrix[1][0])\n",
    "vc_recall = vc_matrix[0][0]*100/(vc_matrix[0][0]+vc_matrix[0][1])\n",
    "\n",
    "print(\"Accuracy : \",vc_accuracy)\n",
    "print(\"Confusion_matrix:\\n\",vc_matrix)\n",
    "# print(\"precision:\",vc_precision)\n",
    "# print(\"recall:\",vc_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>83.380404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XG</th>\n",
       "      <td>86.856398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>87.464697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>87.942646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Tree Classifier</th>\n",
       "      <td>87.269172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting Classifier</th>\n",
       "      <td>87.595047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy\n",
       "Decision Tree          83.380404\n",
       "XG                     86.856398\n",
       "Random Forest          87.464697\n",
       "Logistic Regression    87.942646\n",
       "Extra Tree Classifier  87.269172\n",
       "Voting Classifier      87.595047"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing Accuracies\")\n",
    "acc_list = {\n",
    "    'Decision Tree':dct_accuracy,\n",
    "    'XG':xg_accuracy,\n",
    "    'Random Forest':rf1_accuracy,\n",
    "    'Logistic Regression':lr_accuracy,\n",
    "    'Extra Tree Classifier':etc_accuracy,\n",
    "    'Voting Classifier':vc_accuracy\n",
    "}\n",
    "acc_df_test = pd.DataFrame.from_dict(acc_list,orient=\"index\",columns=['Accuracy'])\n",
    "acc_df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
